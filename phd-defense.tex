% cloned from https://gitlab.kit.edu/kit/kastel/sdq/dokumentvorlagen/praesentationen/beamer
% commit: df83435a0d7e4308ba4115901166d32237c94d67

\documentclass[en, navbarinline, handout]{sdqbeamer}
% remove animation roll-out: handout (general "beamer" option, not specific for this class)
% layout options: 16:9 (default), 16:10, 4:3
% footer font size options: bigfoot (default), smallfoot (KIT layout)
% navigation bar options: navbarinline (default), navbarinfooter, navbarside, navbaroff, navbarkit (off + smallfoot)
% language: de (default), en

\titleimage{title_image}

\grouplogo{}

\groupname{PhD Defense}
%\groupnamewidth{50mm} % default

\title[Leveraging Constraints for User-Centric Feature Selection]{Leveraging Constraints for User-Centric Feature Selection} % [footer]{title slide}
\subtitle{PhD Defense}
\author[Jakob Bach]{Jakob Bach} % [footer]{title slide}

\date[2025-01-20]{January 20, 2025} % [footer]{title slide}

%\usepackage{amsmath} % mathematical symbols and equations; apparently pre-loaded
%\usepackage{amssymb} % mathematical symbols; apparently pre-loaded
\usepackage[style=numeric, backend=biber]{biblatex}
%\usepackage{graphicx} % plots; apparently pre-loaded
%\usepackage{hyperref} % links and URLs; apparently pre-loaded

\addbibresource{references.bib}

\hypersetup{colorlinks=true, citecolor=kit-blue, linkcolor=kit-blue, urlcolor=kit-blue}

\setlength{\leftmargini}{0.2cm} % change default indentation (so items are left-aligned to boxes)
\setlength{\leftmarginii}{0.3cm} % 2nd level indentation
\setlength{\leftmarginiii}{0.3cm} % 3rd level indentation

\setbeamerfont{itemize/enumerate subsubbody}{size=\small} % make 3rd-level items as large as 2nd-level ones (default is \footnotesize, as defined in "beamerfontthemedefault.sty")

\setbeamercovered{invisible} % use "transparent" to show later content of animated slide in gray

\begin{document}

\KITtitleframe

\section{Introduction}

\begin{frame}[t]{Background and Motivation}
	\begin{definition}[Feature selection]
		Given a dataset $X \in \mathbb{R}^{m \times n}$
		and a prediction target $y \in Y^m$ (e.g., $Y = \mathbb{R}$ or $Y = \{0, 1\}$),
		make a feature-selection decision $s \in \{0,1\}^n$
		to optimize the feature-set quality $Q(s,X,y)$.
		Typically, select a fixed number of features~$k \in \mathbb{N}$, i.e., $\sum_{j=1}^n s_j = k$.
	\end{definition}
	%JB: want to clarify which scenario we talk about
	%JB: very generally, we are in field of data science / machine learning
	%JB: focus on supervised feature selection for tabular data
	%JB: examples for prediction target from our experiments (though our methods are domain-independent): is person credit-worthy? is e-mail spam? is mushroom poisonous? will horse survive illness?
	%JB: feature engineering already done (e.g., in spam example)
	%JB: different white-box and black-box objectives for quality, depending on feature-selection method (from correlating each feature with the target to running a genetic algorithm wrapped around a prediction model)
	\pause
	\vspace{\baselineskip}
	\begin{itemize}
		\item Reasons for feature selection~\cite{chandrashekar2014survey, li2017feature}:
		\begin{itemize}
			\item Reduce computational requirements (CPU, memory, storage) of machine learning
			%JB: training and prediction
			\item Improve prediction performance
			% JB: IMHO questionable
			\item Increase interpretability of predictions
		\end{itemize}
		\pause
		\vspace{\baselineskip}
		\item Main limitations of most existing feature-selection methods:
		\begin{itemize}
			\item Do not consider domain knowledge
			%JB: Q() is a technical criterion, and there are typically no constraints apart from cardinality
			%JB: technically good feature sets may still be hard to interpret if they don't make sense from domain perspective
			%JB: domain knowledge may be firm, hypotheses, preferences
			%JB: in literature, there are at most approaches integrating specific constraint types into specific FS methods (apart from the work of Groves)
			\item Return only one feature set, no alternatives
			%JB: may be misleading if there are alternative solutions with similar quality
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Approach}
	\begin{itemize}
		\item Central idea of dissertation: Make feature selection more user-centric via constraints
		%JB: we use constraints in propositional logic and linear arithmetic
		\begin{itemize}
			\item Still optimize feature-set quality but restrict valid feature selections
			\item Formulate as white-box optimization problem and use (MIP/SMT) solver
		\end{itemize}
		\pause
		%
		\begin{example}[A feature-selection constraint in propositional logic]
			$(\lnot s_1 \land \lnot s_2 \land \lnot s_3) \lor (s_1 \land s_2 \land s_3) \leftrightarrow$ ``Select none or all of Features 1, 2, and 3.''
		\end{example}
		%
		\pause
		\vspace{0.5\baselineskip}
		\item Benefits of our approach:
		\begin{itemize}
			\item Declarative
			\item Allows combining constraints
			\item Orthogonal to choice of feature-selection method
			%JB: related work typically integrates one constraint type into one feature-selection method
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Contributions}
	\label{slide:contributions}
	\begin{itemize}
		\item Four core contributions of our dissertation:
		%JB: correspond to four chapters in main part
		\begin{itemize}
			\item (C1) Evaluating the impact of constraints \cite{bach2022empirical}
			\item (C2) Using constraints to formulate scientific hypotheses \cite{bach2022empirical}
			\item (C3) Using constraints for alternative feature sets \cite{bach2023finding, bach2024alternative}
			\item (C4) Using constraints for feature selection in subgroup discovery \cite{bach2025subgroup, bach2024using}
		\end{itemize}
		\item Sub-contributions like formalization, (complexity) analyses, and experimental studies
		%JB: presentation will focus on (C3) and (C4)
		\pause
		\vspace{\baselineskip}
		%JB: Meta-contibution:
		\item Reproducibility:
		\begin{itemize}
			\item All experimental data available on \href{https://doi.org/10.35097/4kjyeg0z2bxmr6eh}{RADAR4KIT}
			\item All code available:
			\begin{itemize}
				\item Three GitHub repositories [\href{https://github.com/Jakob-Bach/Constrained-Filter-Feature-Selection}{a}, \href{https://github.com/Jakob-Bach/Alternative-Feature-Selection}{b}, \href{https://github.com/Jakob-Bach/Constrained-Subgroup-Discovery}{c}]
				\item Three Python packages on PyPI: \href{https://pypi.org/project/alfese/}{\texttt{alfese}}, \href{https://pypi.org/project/cffs/}{\texttt{cffs}}, and \href{https://pypi.org/project/csd/}{\texttt{csd}}
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{frame}

\section{Constrained Feature Selection}

\begin{frame}[t]{Evaluating the Impact of Constraints}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}

\section{Alternative Feature Selection}

\begin{frame}[t]{Finding Alternative Feature Sets}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}

\section{Constrained Subgroup Discovery}

\begin{frame}[t]{Finding Sparse and Alternative Subgroup Descriptions}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}[t]{Conclusions}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}

\appendix
\beginbackup % subsequent slides do not impact overall slide count

\begin{frame}[t, allowframebreaks]{References}
	\printbibliography
\end{frame}

\section{Appendix}

\begin{frame}[t]{Details of Underlying Publications}
	\begin{itemize}
		\item Constrained feature selection (\hyperlink{slide:contributions}{C1} and \hyperlink{slide:contributions}{C2}):
		\begin{itemize}
			\item \fullcite{bach2022empirical}
		\end{itemize}
		\item Alternative feature selection (\hyperlink{slide:contributions}{C3}):
		\begin{itemize}
			\item \fullcite{bach2023finding}
			\item \fullcite{bach2024alternative}
		\end{itemize}
		\item Constrained subgroup discovery (\hyperlink{slide:contributions}{C4}):
		\begin{itemize}
			\item \fullcite{bach2024using}
			\item \fullcite{bach2025subgroup}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Related Work}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}

\backupend

\end{document}
